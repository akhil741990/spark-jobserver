spark {
  master = "spark://ampool-HP-EliteBook-850-G5:7077"

  submit.deployMode = "cluster"

  job-number-cpus = 4

  jobserver {
    port = 8090

    context-per-jvm = true
    sqldao {

      slick-driver = slick.driver.MySQLDriver

      jdbc-driver = com.mysql.jdbc.Driver
      jdbc {
        url = "jdbc:mysql://localhost/spark_jobserver"
        user = "jobserver" # need to create this mysql user on the server
        password = "secret"
      }

      dbcp {
        maxactive = 20
        maxidle = 10
        initialsize = 10
      }
    }
    result-chunk-size = 1m
  }

  context-settings {
    num-cpu-cores = 2           # Number of cores to allocate.  Required.
    memory-per-node = 512m         # Executor memory per node, -Xmx style eg 512m, #1G, etc.

    passthrough {
    }
  }

}

flyway.locations="db/mysql/migration"

akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"
    warn-about-java-serializer-usage = off
    serializers {
      customStartJobSerializer = "spark.jobserver.util.StartJobSerializer"
    }
    serialization-bindings {
      "spark.jobserver.JobManagerActor$StartJob" = customStartJobSerializer
    }
  }

  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = "ampool-HP-EliteBook-850-G5" # on worker node comment this line and on master node add hostname of the master. 
      port = 0
      send-buffer-size = 20 MiB
      receive-buffer-size = 20 MiB
      # This controls the maximum message size, including job results, that can be sent
      maximum-frame-size = 10 MiB
    }
  }

  cluster {
    auto-down-unreachable-after = 20s
    metrics.enabled = off
    failure-detector.acceptable-heartbeat-pause = 3s
    failure-detector.threshold = 12.0
  }
}
